# NLP with Disaster Tweets

Dataset Source: [Kaggle - NLP with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview)

GloVe Embeddings: [Stanford GloVe Pretrained Word Vectors](https://nlp.stanford.edu/projects/glove/)

Approach inspired by basic Keras CNN tutorials and Kaggle discussions on text classification. GloVe embeddings were used to give the model a better starting point in understanding word meanings.  
This helped improve accuracy without needing to train word embeddings from scratch.
